//@version=6
// Library must be declared with overlay=true so it can be imported consistently
library("LogisticModelLib", true)
// Unified logistic regression helpers. This library replaces the
// previous `logistic_regression_utils` and `logistic_training_utils` modules.
//
// This version no longer halts execution using `runtime.error` when invalid
// training parameters are supplied. Instead a lightweight alert is issued and
// the returned `TrainResult` contains an `ok` flag set to `false`.

// -----------------------------------------------------------------------------
// Basic logistic function used by the model
// -----------------------------------------------------------------------------
// Extended logistic function supporting a third feature (e.g. volume).
export logistic(float x1, float x2, float x3, float w0, float w1, float w2, float w3) =>
    float z = w0 + w1 * x1 + w2 * x2 + w3 * x3
    float zClamped = math.min(math.max(z, -50), 50)
    1.0 / (1.0 + math.exp(-zClamped))

// -----------------------------------------------------------------------------
// Logistic regression loss utility
// -----------------------------------------------------------------------------
export logLoss(float y, float p) =>
    float eps = 1e-10
    float safeP = math.max(eps, math.min(p, 1 - eps))
    -y * math.log(safeP) - (1.0 - y) * math.log(1.0 - safeP)

// -----------------------------------------------------------------------------
// Result object returned by the training function
// -----------------------------------------------------------------------------
export type TrainResult
    float w0
    float w1
    float w2
    float w3
    float loss
    bool  ok

// -----------------------------------------------------------------------------
// gradientDescent
// Executes simple gradient descent on the logistic regression model.
// -----------------------------------------------------------------------------
// Optional initial weights allow continuing training from a previous state.
// `run` should be `true` only on the bar that triggers training (e.g.,
// `barstate.islast` or a custom flag). This avoids executing the expensive
// epoch/sample loops every bar.
export gradientDescent(
     float[] x1,
     float[] x2,
     float[] x3,
     float[] y,
     float rate,
     int   epochs,
     float w0Init = 0.0,
     float w1Init = 0.0,
     float w2Init = 0.0,
     float w3Init = 0.0,
     bool  run    = true
 ) =>
    if not run
        // Training skipped because `run` is false.
        // Returns previous weights and marks result as not ok.
        TrainResult.new(w0Init, w1Init, w2Init, w3Init, na, false)
    else
        // Use the smallest array length to avoid out-of-range errors
        int n = math.min(array.size(x1), math.min(array.size(x2), math.min(array.size(x3), array.size(y))))
        bool success = true
        if array.size(x1) != array.size(x2) or array.size(x2) != array.size(x3) or array.size(x3) != array.size(y)
            alert("gradientDescent: input array sizes differ; using min length " + str.tostring(n), alert.freq_once_per_bar)
            success := false
        if n == 0
            alert("gradientDescent: no training data provided", alert.freq_once_per_bar)
            TrainResult.new(w0Init, w1Init, w2Init, w3Init, 0.0, false)
        else
            float w0 = w0Init
            float w1 = w1Init
            float w2 = w2Init
            float w3 = w3Init
            float loss = 0.0
            if epochs <= 0
                alert("gradientDescent: epochs must be > 0", alert.freq_once_per_bar)
                TrainResult.new(w0, w1, w2, w3, 0.0, false)
            else
                for ep = 0 to epochs - 1
                    float epochLoss = 0.0
                    for i = 0 to n - 1
                        float f1 = array.get(x1, i)
                        float f2 = array.get(x2, i)
                        float f3 = array.get(x3, i)
                        float label = array.get(y, i)
                        float pred = logistic(f1, f2, f3, w0, w1, w2, w3)
                        float error = pred - label
                        w0 -= rate * error
                        w1 -= rate * error * f1
                        w2 -= rate * error * f2
                        w3 -= rate * error * f3
                        epochLoss += logLoss(label, pred)
                    if n > 0
                        epochLoss /= n
                    loss := epochLoss
            TrainResult.new(w0, w1, w2, w3, loss, success)
